import randomimport osfrom deap import basefrom deap import creatorfrom deap import toolsimport _config_ea_searchfrom methods import hyperparametersimport sklearn.model_selection as model_selectionfrom sklearn.model_selection import cross_val_scorefrom numpy import std, nan, isnanimport searchimport database__all__ = ['EvolutionSearch', 'EvolutionSearchCV']'''    Mutation functions'''def uniform(individual, indpb, param_names, hyperparameter_space, verbose=0):            i = 0    old_individual = []    old_individual[:] = individual    changed = False    for param_name in param_names:        if random.random() < indpb:            changed = True            # find param in hyperparameter_space            param = hyperparameter_space.get_hyperparameter_by_name(param_name)                                                                if param.__class__.__name__ == "FloatHyperparameter":                            new_value = random.uniform(param.lower, param.upper)            if param.__class__.__name__ == "CategoricalHyperparameter":                new_value = random.choice(param.value)            if param.__class__.__name__ == "IntegerHyperparameter":                new_value = random.randint(param.lower, param.upper)            individual[i] = new_value        else:            pass        i += 1             if changed and verbose > 0:        print ("Mutation (uniform):", old_individual, "=>", individual)                            return individual,def gaussian(individual, indpb, param_names, hyperparameter_space, verbose=0):        i = 0    old_individual = []    old_individual[:] = individual    changed = False    for param_name in param_names:        if random.random() < indpb:            changed = True            # find param in hyperparameter_space            param = hyperparameter_space.get_hyperparameter_by_name(param_name)                                                                if param.__class__.__name__ == "FloatHyperparameter":                          sigma = (0.34*(param.upper-param.lower))                  mean = (param.lower+param.upper)/2 # average                new_value = random.gauss(mean, sigma )            if param.__class__.__name__ == "CategoricalHyperparameter":                new_value = random.choice(param.value)            if param.__class__.__name__ == "IntegerHyperparameter":                                sigma = (0.34*(param.upper-param.lower))                  mean = (param.lower+param.upper)/2 # average                new_value = random.gauss(mean, sigma)                new_value = round(new_value)            individual[i] = new_value                    else:            # leave the old value             pass         i += 1     if changed and verbose > 0:        print ("Mutation (gaussian):", old_individual, "=>", individual)                                return individual,def random_mutation_operator(individual, mutation_operators, param_names, hyperparameter_space, verbose):    '''        Randomly picks one of the mutation operators from a config file.    '''      operator = random.choice(mutation_operators)    name = operator[0]    prob = operator[1]    operator_function = eval(name)    return operator_function(individual, prob, param_names, hyperparameter_space, verbose)        '''    EvolutionarySearch class'''class EvolutionarySearch(search.Search):    def __init__(self, estimator, chain_names, hyperparameter_space, dataset_name, verbose=0):                        super(EvolutionarySearch, self).__init__()                self.estimator = estimator        self.chain_names = chain_names        self.hyperparameter_space = hyperparameter_space        self.dataset_name = dataset_name        self.verbose = verbose                self.evolution_init(hyperparameter_space)                # TODO parallel crossvalidation!!!                        def get_name(self):        return "Evolutionary search (deap)"          def get_params(self, individual):        params = {}        for p in self.params:            params[p] = individual[0]            individual = individual[1:]        return params                def evalFitnessNTimes(self, individual, n):        score = 0                                        try:        # n = 10            for k in range(n):                            self.estimator.__init__()                 # z = {**x, **y}                params = {**self.get_params(individual), **self.constant_params}                 self.estimator.set_params(**params)                            self.estimator.fit(self.X_train, self.y_train)                s = self.estimator.score(self.X_test, self.y_test)                # print(s)                score += s            score = score / n          except Exception as e:                  print("Exception: "+str(e))                            #print()        #print("SCORE: ", score)        return score,    def evalFitness(self, individual):        return self.evalFitnessNTimes(individual, 1)                         def evolution_init(self, hyperparameter_space):                #----------        # Evolution parameters        #----------                # CXPB  is the probability with which two individuals        #       are crossed        #        # MUTPB is the probability for mutating an individual        #        # NGEN  is the number of generations for which the        #       evolution runs        #        # NPOP  us the number of individuals in the population                self.CXPB = _config_ea_search.CXPB        self.MUTPB = _config_ea_search.MUTPB        self.NGEN = _config_ea_search.NGEN        self.NPOP = _config_ea_search.NPOP                # Klara's param, not from deap        self.elitism = _config_ea_search.elitism                        # TODO logaritmicka mutace - linearni na logaritmu cisla /         # nasobeni / deleni                         if self.verbose > 0:            print("CXPB:", self.CXPB)            print("MUTPB:", self.MUTPB)            print("NGEN:", self.NGEN)            print("NPOP:", self.NPOP)                    #random.seed(64)  # TODO                        creator.create("FitnessMax", base.Fitness, weights=(1.0,))        creator.create("Individual", list, fitness=creator.FitnessMax)                                self.toolbox = base.Toolbox()                                #----------        # Define the population / individuals        #----------                # For each parameter register a 'attr'        # e.g. self.toolbox.register("attr_bool", random.randint, 0, 1)                # generate attrs automatically                               self.params = []        self.constant_params = {}                for param in self.hyperparameter_space.hyperparameters:                                    if param.__class__.__name__ == "FloatHyperparameter":                            self.toolbox.register("attr_"+param.name, random.uniform,                                       param.lower, param.upper)            if param.__class__.__name__ == "CategoricalHyperparameter":                self.toolbox.register("attr_"+param.name, random.choice,                                       param.value)            if param.__class__.__name__ == "IntegerHyperparameter":                self.toolbox.register("attr_"+param.name, random.randint,                                      param.lower, param.upper)            if param.__class__.__name__ == "Constant":                self.constant_params[param.name] = param.value                # ... there is only one value for the constant parameter                  continue            self.params += [param.name]                                                        attrs = ["self.toolbox.attr_"+a for a in self.params]        if self.verbose > 0:            print("Attributes:", self.params)            print("Constant attributes:", self.constant_params)                # we need a tuple of attrs        attrs_tupple_str = "("+", ".join(attrs)+")"                self.toolbox.register("individual", tools.initCycle, creator.Individual,                         eval(attrs_tupple_str))                         # Define the population to be a list of individuals        self.toolbox.register("population", tools.initRepeat, list,                               self.toolbox.individual)                        #----------        # Operator registration        #----------                # register the goal / fitness function        self.toolbox.register("evaluate", self.evalFitness)                # register the crossover operator        self.toolbox.register("mate", _config_ea_search.crossover)                # register a mutation operator              self.toolbox.register('mutate', random_mutation_operator, mutation_operators=_config_ea_search.mutation_operators, param_names=self.params, hyperparameter_space=self.hyperparameter_space, verbose=self.verbose)                   # operator for selecting individuals for breeding the next        # generation: each individual of the current generation        # is replaced by the 'fittest' (best) of three individuals        # drawn randomly from the current generation.        self.toolbox.register("select", tools.selTournament, tournsize=2)        if self.verbose > 0:                        print("crossover:", _config_ea_search.crossover.__name__)            for m in _config_ea_search.mutation_operators:                print("mutation:", m[0], "prob:", m[1])                                    #----------        # Create an initial population of NPOP individuals        #----------                 self.pop = self.toolbox.population(n=self.NPOP)           def predict(self, X):        """Call predict on the estimator with the best found parameters."""        return self.best_estimator.predict(X)      def fit(self, X, y, groups=None):        self.X = X        self.y = y           self.groups = groups                   # Split the dataset into testing and training data        self.X_train, self.X_test, self.y_train, self.y_test = \          self.split_dataset(X, y)                                print("Start of evolution")                        # Evaluate the entire self.population        fitnesses = list(map(self.toolbox.evaluate, self.pop))        for ind, fit in zip(self.pop, fitnesses):            _fit = fit[0]            if isnan(_fit):                _fit = 0            ind.fitness.values = (_fit,)  # mean_error or 0            ind.mean = fit[0]            ind.std = fit[1]                                           print("  Evaluated %i individuals" % len(self.pop))        print()                # write results to the file        f, file_name = self.results_file_open("w")                                        self.write_header(f)                f.write("# Evolution Parameters: "+"\n"+                "#     CXPB:"+str(self.CXPB)+"\n"+                "#     MUTPB:"+str(self.MUTPB)+"\n"+                "#     NGEN:"+str(self.NGEN)+"\n"+                "#     NPOP:"+str(self.NPOP)+"\n")                      f.write("#\n#\n")        f.write("# (max , min, mean, std)\n")        f.write("#\n")        f.close()                # Begin the evolution        for g in range(self.NGEN):                    print("-- Generation %i --" % g)                                                                        # Save elite for later            elite = self.pop[-self.elitism:]                        # Select the next generation individuals            offspring = self.toolbox.select(self.pop, len(self.pop))            # Clone the selected individuals            offspring = list(map(self.toolbox.clone, offspring))            # Apply crossover and mutation on the offspring                        for child1, child2 in zip(offspring[::2], offspring[1::2]):                                                # cross two individuals with probability CXPB                if random.random() < self.CXPB:                    self.toolbox.mate(child1, child2)                        # fitness values of the children                    # must be recalculated later                    del child1.fitness.values                    del child2.fitness.values                for mutant in offspring:                    # mutate an individual with probability MUTPB                if random.random() < self.MUTPB:                    self.toolbox.mutate(mutant)                    del mutant.fitness.values                    # Return elite to the population            offspring = offspring[:-self.elitism]+elite                        # Evaluate the individuals with an invalid fitness            invalid_ind = [ind for ind in offspring if not ind.fitness.valid]            fitnesses = map(self.toolbox.evaluate, invalid_ind)                        mean_std = []  # list of tuples            for ind, fit in zip(invalid_ind, fitnesses):                _fit = fit[0]                if isnan(_fit):                    _fit = 0                ind.fitness.values = (_fit,)  # mean_error or 0                ind.mean = fit[0]                ind.std = fit[1]                        print("  Evaluated %i individuals" % len(invalid_ind))                                    # Find new elite (after the evaluation)            elite = tools.selBest(self.pop, self.elitism)                        # The self.population is entirely replaced by the offspring            # + the new elite                                    self.pop[:] = offspring[:-self.elitism]+elite                                                                                        best_ind = tools.selBest(self.pop, 1)[0]                print("Best individual is %s, %s" % (best_ind, best_ind.fitness.values))                                                # Keep the same size of population (NPOP)            # If the population size < NPOP, copy some individuals            while len(self.pop) < self.NPOP:                            self.pop += [random.choice(self.pop)]            if self.verbose > 1:                       print("Population size:", len(self.pop))                         # Gather all the fitnesses in one list and print the stats            fits = [ind.fitness.values for ind in self.pop]                                                fits = [f[0] for f in fits if f[0] > 0]            length = len(fits)                        mean = sum(fits) / length            sum2 = sum(x*x for x in fits)            std = abs(sum2 / length - mean**2)**0.5                                    #print("  Min %s" % min(fits))            #print("  Max %s" % max(fits))            #print("  Avg %s" % mean)            #print("  Std %s" % std)                        ''' write the best individual to a file '''            f = open(file_name, "a")                                 f.write(str(max(fits))+" ,"                +str(min(fits))+","                +str(mean)+","                +str(std)                +"\n")            f.close()                            ''' write all individuals to a file                 write results to a database            '''            f1 = open(file_name+".ind", "a")             f1.write("\n# -- Generation %i --\n" % g)                                                            results = []            for ind in self.pop:                                            # desired output: 0.375 (+/-0.113) for {'decision_tree__criterion': 'gini', ...                                std = ind.std * 2                f1.write(str(ind.mean)                    +" (+/-"+str(std)+")"                    +" for "                    +str(self.get_params(ind))                                        +"\n")                                                     #val = [                # ('', 'test_method', 'test params', 'test datset', 0.567, 0.2)                #]                    results.append((self.experiment_id, self.get_name(), str(self.chain_names[:-1]), str(self.chain_names[-1]), str(self.get_params(ind)), self.cv, self.dataset_name, float(ind.mean), float(ind.std)))                                                      f1.close()                                                    try:                database.insert_results(results)            except Exception as e:                # print(results)                print(e)                            # End the evolution if optimal solution is found:            if max(fits) == 1:              break                    print()                print("-- End of (successful) evolution --")                best_ind = tools.selBest(self.pop, 1)[0]        print("Best individual is %s, %s" % (best_ind, best_ind.fitness.values))                        self.best_params_ = self.get_params(best_ind)               self.best_estimator = self.estimator.set_params(**self.best_params_)                  f = open(file_name, "a")          f.write("# Best individual: "+str(self.best_params_)+                ", fitness: "+str(best_ind.fitness.values))        f.close()        class EvolutionarySearchCV(EvolutionarySearch):    """    n_jobs : int, default=1        Number of jobs to run in parallel.    pre_dispatch : int, or string, optional        Controls the number of jobs that get dispatched during parallel        execution. Reducing this number can be useful to avoid an        explosion of memory consumption when more jobs get dispatched        than CPUs can process. This parameter can be:            - None, in which case all the jobs are immediately              created and spawned. Use this for lightweight and              fast-running jobs, to avoid delays due to on-demand              spawning of the jobs            - An int, giving the exact number of total jobs that are              spawned            - A string, giving an expression as a function of n_jobs,              as in '2*n_jobs'    """    def __init__(self, estimator, chain_names, hyperparameter_space, dataset_name,                 verbose=0, n_jobs=1, pre_dispatch='2*n_jobs', scoring=None):                    EvolutionarySearch.__init__(self, estimator, chain_names, hyperparameter_space,                                  dataset_name, verbose=verbose)                self.cv = _config_ea_search.cv_folds        self.n_jobs = n_jobs        self.pre_dispatch = pre_dispatch        self.scoring = scoring        self.chain_names = chain_names    def evalFitness(self, individual):         # evaluate the given estimator+params using cv        score = 0        if self.verbose > 1:            print("-- evalFitness")                     try:                    # self.estimator.__init__()             params = {**self.get_params(individual), **self.constant_params}            if self.verbose > 1:                print("Params:", params)             self.estimator.set_params(**params)                                        scores = cross_val_score(self.estimator, self.X, self.y, cv=self.cv)            # scores ... array of scores => count average            if self.verbose > 2:                print("---- scores:", scores)                        mean_score = sum(scores)/self.cv            std_score = std(scores)                             if self.verbose > 1:                print("---- avg score:", score)            except Exception as e:           # TODO: odchytavat jenom tu spravnou vyjimku !!!                 print("Exception: "+str(e))           mean_score = nan           std_score = nan                                    # return score,        return mean_score, std_score                        def evalFitnessParallel(self, individual):         # !!! TODO                 params = {**self.get_params(individual), **self.constant_params}         self.estimator.set_params(**params)                estimator = self.estimator                        cv = check_cv(self.cv, self.y, classifier=is_classifier(estimator))        X, y, groups = indexable(self.X, self.y, self.groups)        n_splits = cv.get_n_splits(X, y, groups)        # Regenerate parameter iterable for each fit        candidate_params = params        n_candidates = len(candidate_params)        if self.verbose > 0:            print("Fitting {0} folds for each of {1} candidates, totalling"                  " {2} fits".format(n_splits, n_candidates,                                     n_candidates * n_splits))        base_estimator = clone(self.estimator)        pre_dispatch = self.pre_dispatch        out = Parallel(            n_jobs=self.n_jobs, verbose=self.verbose,            pre_dispatch=pre_dispatch        )(delayed(_fit_and_score)(clone(base_estimator), X, y, scorers, train,                                  test, self.verbose, parameters,                                  fit_params=fit_params,                                  return_train_score=self.return_train_score,                                  return_n_test_samples=True,                                  return_times=True, return_parameters=False,                                  error_score=self.error_score)          for parameters, (train, test) in product(candidate_params,                                                   cv.split(X, y, groups)))                                                                                        '''# testfrom sklearn.neighbors import KNeighborsClassifierhs = hyperparameters.HyperparameterSpace()hs.add_hyperparameter(hyperparameters.IntegerHyperparameter(name="n_neighbors",                      lower=1, upper=100, default=1))hs.add_hyperparameter(hyperparameters.CategoricalHyperparameter(name="weights",                      choices=["uniform", "distance"], default="uniform"))hs.add_hyperparameter(hyperparameters.CategoricalHyperparameter(name="p",                       choices=[1, 2], default=2))hs.add_hyperparameter(hyperparameters.FloatHyperparameter("t", 0.5, 1.5,                       default=1))         EvolutionSearch(KNeighborsClassifier, hs, "test_dataset", verbose=1)'''